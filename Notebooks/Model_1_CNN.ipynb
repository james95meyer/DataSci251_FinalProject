{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "086f7336",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import shutil\n",
    "import time\n",
    "import warnings\n",
    "import soundfile\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "\n",
    "import torch.utils.data\n",
    "import torchaudio\n",
    "from torchaudio import transforms\n",
    "from IPython.display import Audio\n",
    "\n",
    "torchaudio.set_audio_backend('soundfile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0993e36b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/DataSci251_FinalProject'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80d19ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········································\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/DataSci251_FinalProject/wandb/run-20230411_003327-xbipdmxg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/james95meyer_team/FinalProject_SimpleModel/runs/xbipdmxg' target=\"_blank\">leafy-galaxy-6</a></strong> to <a href='https://wandb.ai/james95meyer_team/FinalProject_SimpleModel' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/james95meyer_team/FinalProject_SimpleModel' target=\"_blank\">https://wandb.ai/james95meyer_team/FinalProject_SimpleModel</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/james95meyer_team/FinalProject_SimpleModel/runs/xbipdmxg' target=\"_blank\">https://wandb.ai/james95meyer_team/FinalProject_SimpleModel/runs/xbipdmxg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/james95meyer_team/FinalProject_SimpleModel/runs/xbipdmxg?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f1cf98e1670>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"FinalProject_SimpleModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acbfa9cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1-100032-A-0.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1-100038-A-14.wav</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1-100210-A-36.wav</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1-100210-B-36.wav</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1-101296-A-19.wav</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            filename  target\n",
       "0   1-100032-A-0.wav       0\n",
       "1  1-100038-A-14.wav      14\n",
       "2  1-100210-A-36.wav      36\n",
       "3  1-100210-B-36.wav      36\n",
       "4  1-101296-A-19.wav      19"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading Data and CSV\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "download_path = Path('DataSet/ESC-50-master')\n",
    "\n",
    "# Read metadata file\n",
    "metadata_file = download_path/'meta'/'esc50.csv'\n",
    "df = pd.read_csv(metadata_file)\n",
    "df.head()\n",
    "\n",
    "# Take relevant columns\n",
    "df = df[['filename', 'target']]\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01efe76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Data Transformations\n",
    "\n",
    "class AudioUtil():\n",
    "    \n",
    "  # Load an audio file. Return the signal as a tensor and the sample rate\n",
    "  @staticmethod\n",
    "  def open(audio_file):\n",
    "    sig, sr = torchaudio.load(audio_file)\n",
    "    return (sig, sr)\n",
    "\n",
    "  # Convert the given audio to the desired number of channels\n",
    "  @staticmethod\n",
    "  def rechannel(aud, new_channel):\n",
    "    sig, sr = aud\n",
    "\n",
    "    if (sig.shape[0] == new_channel):\n",
    "      # Nothing to do\n",
    "      return aud\n",
    "\n",
    "    if (new_channel == 1):\n",
    "      # Convert from stereo to mono by selecting only the first channel\n",
    "      resig = sig[:1, :]\n",
    "    else:\n",
    "      # Convert from mono to stereo by duplicating the first channel\n",
    "      resig = torch.cat([sig, sig])\n",
    "\n",
    "    return ((resig, sr))\n",
    "\n",
    "  # Resample for proper channels\n",
    "  @staticmethod\n",
    "  def resample(aud, newsr):\n",
    "    sig, sr = aud\n",
    "\n",
    "    if (sr == newsr):\n",
    "      # Nothing to do\n",
    "      return aud\n",
    "\n",
    "    num_channels = sig.shape[0]\n",
    "    # Resample first channel\n",
    "    resig = torchaudio.transforms.Resample(sr, newsr)(sig[:1,:])\n",
    "    if (num_channels > 1):\n",
    "      # Resample the second channel and merge both channels\n",
    "      retwo = torchaudio.transforms.Resample(sr, newsr)(sig[1:,:])\n",
    "      resig = torch.cat([resig, retwo])\n",
    "\n",
    "    return ((resig, newsr))\n",
    "\n",
    "  # Pad the signal to a fixed length 'max_ms' in milliseconds\n",
    "  @staticmethod\n",
    "  def pad_trunc(aud, max_ms):\n",
    "    sig, sr = aud\n",
    "    num_rows, sig_len = sig.shape\n",
    "    max_len = sr//1000 * max_ms\n",
    "\n",
    "    if (sig_len > max_len):\n",
    "      # Truncate the signal to the given length\n",
    "      sig = sig[:,:max_len]\n",
    "\n",
    "    elif (sig_len < max_len):\n",
    "      # Length of padding to add at the beginning and end of the signal\n",
    "      pad_begin_len = random.randint(0, max_len - sig_len)\n",
    "      pad_end_len = max_len - sig_len - pad_begin_len\n",
    "\n",
    "      # Pad with 0s\n",
    "      pad_begin = torch.zeros((num_rows, pad_begin_len))\n",
    "      pad_end = torch.zeros((num_rows, pad_end_len))\n",
    "\n",
    "      sig = torch.cat((pad_begin, sig, pad_end), 1)\n",
    "      \n",
    "    return (sig, sr)\n",
    "\n",
    "  # Shift the signal to the left or right by some percent\n",
    "  @staticmethod\n",
    "  def time_shift(aud, shift_limit):\n",
    "    sig,sr = aud\n",
    "    _, sig_len = sig.shape\n",
    "    shift_amt = int(random.random() * shift_limit * sig_len)\n",
    "    return (sig.roll(shift_amt), sr)\n",
    "\n",
    "  # Generate a Spectrogram\n",
    "  @staticmethod\n",
    "  def spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None):\n",
    "    sig,sr = aud\n",
    "    top_db = 80\n",
    "\n",
    "    # spec has shape [channel, n_mels, time], where channel is mono, stereo etc\n",
    "    spec = transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig)\n",
    "\n",
    "    # Convert to decibels\n",
    "    spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n",
    "    return (spec)\n",
    "\n",
    "  # Augment the Spectrogram to prevent overfitting\n",
    "  @staticmethod\n",
    "  def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n",
    "    _, n_mels, n_steps = spec.shape\n",
    "    mask_value = spec.mean()\n",
    "    aug_spec = spec\n",
    "\n",
    "    freq_mask_param = max_mask_pct * n_mels\n",
    "    for _ in range(n_freq_masks):\n",
    "      aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n",
    "\n",
    "    time_mask_param = max_mask_pct * n_steps\n",
    "    for _ in range(n_time_masks):\n",
    "      aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n",
    "\n",
    "    return aug_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c1db34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Path\n",
    "data_path = Path('DataSet/ESC-50-master/audio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd2fb425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0089, 0.0089, 0.0089,  ..., 0.0034, 0.0038, 0.0038]]), 44100)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchaudio.load(data_path/'1-137-A-32.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2149ba03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating data set\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchaudio\n",
    "\n",
    "class SoundDS(Dataset):\n",
    "  def __init__(self, df, data_path):\n",
    "    self.df = df\n",
    "    self.data_path = str(data_path)\n",
    "    self.duration = 4000\n",
    "    self.sr = 44100\n",
    "    self.channel = 2\n",
    "    self.shift_pct = 0.4\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.df)    \n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    audio_file = self.data_path + '/' + self.df.loc[idx, 'filename']\n",
    "    # Get the Class ID\n",
    "    class_id = self.df.loc[idx, 'target']\n",
    "\n",
    "    aud = AudioUtil.open(audio_file)\n",
    "    reaud = AudioUtil.resample(aud, self.sr)\n",
    "    rechan = AudioUtil.rechannel(reaud, self.channel)\n",
    "\n",
    "    dur_aud = AudioUtil.pad_trunc(rechan, self.duration)\n",
    "    shift_aud = AudioUtil.time_shift(dur_aud, self.shift_pct)\n",
    "    sgram = AudioUtil.spectro_gram(shift_aud, n_mels=64, n_fft=1024, hop_len=None)\n",
    "    aug_sgram = AudioUtil.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n",
    "\n",
    "    return aug_sgram, class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e87d031c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating data loaders\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "myds = SoundDS(df, data_path)\n",
    "\n",
    "# Random split of 80:20 between training and validation\n",
    "num_items = len(myds)\n",
    "num_train = round(num_items * 0.8)\n",
    "num_val = num_items - num_train\n",
    "train_ds, val_ds = random_split(myds, [num_train, num_val])\n",
    "\n",
    "# Create training and validation data loaders\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=8)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d117d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "414c2f0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "\n",
    "# Audio Classification Model\n",
    "\n",
    "class AudioClassifier (nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        conv_layers = []\n",
    "\n",
    "        # First Convolution Block with Relu and Batch Norm. Use Kaiming Initialization\n",
    "        self.conv1 = nn.Conv2d(2, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        init.kaiming_normal_(self.conv1.weight, a=0.1)\n",
    "        self.conv1.bias.data.zero_()\n",
    "        conv_layers += [self.conv1, self.relu1, self.bn1]\n",
    "\n",
    "        # Second Convolution Block\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        init.kaiming_normal_(self.conv2.weight, a=0.1)\n",
    "        self.conv2.bias.data.zero_()\n",
    "        conv_layers += [self.conv2, self.relu2, self.bn2]\n",
    "\n",
    "        # Second Convolution Block\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        init.kaiming_normal_(self.conv3.weight, a=0.1)\n",
    "        self.conv3.bias.data.zero_()\n",
    "        conv_layers += [self.conv3, self.relu3, self.bn3]\n",
    "\n",
    "        # Second Convolution Block\n",
    "        self.conv4 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        init.kaiming_normal_(self.conv4.weight, a=0.1)\n",
    "        self.conv4.bias.data.zero_()\n",
    "        conv_layers += [self.conv4, self.relu4, self.bn4]\n",
    "\n",
    "        # Linear Classifier\n",
    "        self.ap = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.lin = nn.Linear(in_features=64, out_features=50)\n",
    "\n",
    "        # Wrap the Convolutional Blocks\n",
    "        self.conv = nn.Sequential(*conv_layers)\n",
    " \n",
    "    # Forward pass\n",
    "    def forward(self, x):\n",
    "        # Run the convolutional blocks\n",
    "        x = self.conv(x)\n",
    "\n",
    "        # Adaptive pool and flatten for input to linear layer\n",
    "        x = self.ap(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # Linear layer\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # Final output\n",
    "        return x\n",
    "\n",
    "# Create the model and put it on the GPU if available\n",
    "myModel = AudioClassifier()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "myModel = myModel.to(device)\n",
    "\n",
    "# Check that it is on Cuda\n",
    "next(myModel.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75e1729f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 3.91, Acc@1: 0.00, Acc@5: 12.50\n",
      "Epoch: 1, Loss: 3.86, Acc@1: 3.12, Acc@5: 6.25\n",
      "Epoch: 2, Loss: 3.79, Acc@1: 6.25, Acc@5: 34.38\n",
      "Epoch: 3, Loss: 3.72, Acc@1: 18.75, Acc@5: 40.62\n",
      "Epoch: 4, Loss: 3.64, Acc@1: 9.38, Acc@5: 21.88\n",
      "Epoch: 5, Loss: 3.56, Acc@1: 6.25, Acc@5: 28.12\n",
      "Epoch: 6, Loss: 3.49, Acc@1: 9.38, Acc@5: 34.38\n",
      "Epoch: 7, Loss: 3.40, Acc@1: 18.75, Acc@5: 31.25\n",
      "Epoch: 8, Loss: 3.33, Acc@1: 15.62, Acc@5: 34.38\n",
      "Epoch: 9, Loss: 3.27, Acc@1: 21.88, Acc@5: 62.50\n",
      "Epoch: 10, Loss: 3.19, Acc@1: 21.88, Acc@5: 65.62\n",
      "Epoch: 11, Loss: 3.11, Acc@1: 15.62, Acc@5: 43.75\n",
      "Epoch: 12, Loss: 3.04, Acc@1: 21.88, Acc@5: 56.25\n",
      "Epoch: 13, Loss: 2.97, Acc@1: 18.75, Acc@5: 53.12\n",
      "Epoch: 14, Loss: 2.89, Acc@1: 37.50, Acc@5: 75.00\n",
      "Epoch: 15, Loss: 2.83, Acc@1: 18.75, Acc@5: 56.25\n",
      "Epoch: 16, Loss: 2.74, Acc@1: 12.50, Acc@5: 56.25\n",
      "Epoch: 17, Loss: 2.68, Acc@1: 43.75, Acc@5: 75.00\n",
      "Epoch: 18, Loss: 2.60, Acc@1: 28.12, Acc@5: 53.12\n",
      "Epoch: 19, Loss: 2.53, Acc@1: 46.88, Acc@5: 78.12\n",
      "Epoch: 20, Loss: 2.48, Acc@1: 21.88, Acc@5: 65.62\n",
      "Epoch: 21, Loss: 2.42, Acc@1: 34.38, Acc@5: 71.88\n",
      "Epoch: 22, Loss: 2.35, Acc@1: 40.62, Acc@5: 71.88\n",
      "Epoch: 23, Loss: 2.27, Acc@1: 34.38, Acc@5: 68.75\n",
      "Epoch: 24, Loss: 2.21, Acc@1: 31.25, Acc@5: 68.75\n",
      "Epoch: 25, Loss: 2.16, Acc@1: 56.25, Acc@5: 90.62\n",
      "Epoch: 26, Loss: 2.08, Acc@1: 31.25, Acc@5: 71.88\n",
      "Epoch: 27, Loss: 2.05, Acc@1: 40.62, Acc@5: 78.12\n",
      "Epoch: 28, Loss: 2.02, Acc@1: 71.88, Acc@5: 90.62\n",
      "Epoch: 29, Loss: 1.93, Acc@1: 46.88, Acc@5: 81.25\n",
      "Epoch: 30, Loss: 1.92, Acc@1: 59.38, Acc@5: 90.62\n",
      "Epoch: 31, Loss: 1.83, Acc@1: 56.25, Acc@5: 71.88\n",
      "Epoch: 32, Loss: 1.80, Acc@1: 62.50, Acc@5: 84.38\n",
      "Epoch: 33, Loss: 1.75, Acc@1: 37.50, Acc@5: 78.12\n",
      "Epoch: 34, Loss: 1.68, Acc@1: 46.88, Acc@5: 84.38\n",
      "Epoch: 35, Loss: 1.67, Acc@1: 43.75, Acc@5: 78.12\n",
      "Epoch: 36, Loss: 1.63, Acc@1: 56.25, Acc@5: 90.62\n",
      "Epoch: 37, Loss: 1.59, Acc@1: 59.38, Acc@5: 84.38\n",
      "Epoch: 38, Loss: 1.53, Acc@1: 59.38, Acc@5: 90.62\n",
      "Epoch: 39, Loss: 1.52, Acc@1: 68.75, Acc@5: 93.75\n",
      "Epoch: 40, Loss: 1.45, Acc@1: 65.62, Acc@5: 78.12\n",
      "Epoch: 41, Loss: 1.46, Acc@1: 43.75, Acc@5: 87.50\n",
      "Epoch: 42, Loss: 1.42, Acc@1: 62.50, Acc@5: 81.25\n",
      "Epoch: 43, Loss: 1.40, Acc@1: 56.25, Acc@5: 84.38\n",
      "Epoch: 44, Loss: 1.34, Acc@1: 68.75, Acc@5: 96.88\n",
      "Epoch: 45, Loss: 1.35, Acc@1: 53.12, Acc@5: 87.50\n",
      "Epoch: 46, Loss: 1.30, Acc@1: 71.88, Acc@5: 90.62\n",
      "Epoch: 47, Loss: 1.31, Acc@1: 71.88, Acc@5: 93.75\n",
      "Epoch: 48, Loss: 1.29, Acc@1: 56.25, Acc@5: 87.50\n",
      "Epoch: 49, Loss: 1.26, Acc@1: 56.25, Acc@5: 93.75\n",
      "Epoch: 50, Loss: 1.24, Acc@1: 75.00, Acc@5: 90.62\n",
      "Epoch: 51, Loss: 1.21, Acc@1: 53.12, Acc@5: 100.00\n",
      "Epoch: 52, Loss: 1.21, Acc@1: 71.88, Acc@5: 100.00\n",
      "Epoch: 53, Loss: 1.20, Acc@1: 62.50, Acc@5: 93.75\n",
      "Epoch: 54, Loss: 1.15, Acc@1: 62.50, Acc@5: 93.75\n",
      "Epoch: 55, Loss: 1.15, Acc@1: 62.50, Acc@5: 84.38\n",
      "Epoch: 56, Loss: 1.13, Acc@1: 68.75, Acc@5: 96.88\n",
      "Epoch: 57, Loss: 1.11, Acc@1: 68.75, Acc@5: 93.75\n",
      "Epoch: 58, Loss: 1.11, Acc@1: 62.50, Acc@5: 81.25\n",
      "Epoch: 59, Loss: 1.10, Acc@1: 59.38, Acc@5: 90.62\n",
      "Epoch: 60, Loss: 1.09, Acc@1: 65.62, Acc@5: 96.88\n",
      "Epoch: 61, Loss: 1.07, Acc@1: 71.88, Acc@5: 96.88\n",
      "Epoch: 62, Loss: 1.06, Acc@1: 78.12, Acc@5: 96.88\n",
      "Epoch: 63, Loss: 1.01, Acc@1: 62.50, Acc@5: 81.25\n",
      "Epoch: 64, Loss: 1.01, Acc@1: 75.00, Acc@5: 100.00\n",
      "Epoch: 65, Loss: 0.99, Acc@1: 65.62, Acc@5: 100.00\n",
      "Epoch: 66, Loss: 1.03, Acc@1: 81.25, Acc@5: 96.88\n",
      "Epoch: 67, Loss: 0.98, Acc@1: 53.12, Acc@5: 93.75\n",
      "Epoch: 68, Loss: 0.97, Acc@1: 78.12, Acc@5: 100.00\n",
      "Epoch: 69, Loss: 1.01, Acc@1: 56.25, Acc@5: 90.62\n",
      "Epoch: 70, Loss: 0.98, Acc@1: 71.88, Acc@5: 93.75\n",
      "Epoch: 71, Loss: 1.00, Acc@1: 71.88, Acc@5: 96.88\n",
      "Epoch: 72, Loss: 0.95, Acc@1: 68.75, Acc@5: 96.88\n",
      "Epoch: 73, Loss: 0.94, Acc@1: 68.75, Acc@5: 96.88\n",
      "Epoch: 74, Loss: 0.96, Acc@1: 75.00, Acc@5: 93.75\n",
      "Epoch: 75, Loss: 0.93, Acc@1: 68.75, Acc@5: 93.75\n",
      "Epoch: 76, Loss: 0.93, Acc@1: 75.00, Acc@5: 93.75\n",
      "Epoch: 77, Loss: 0.90, Acc@1: 84.38, Acc@5: 100.00\n",
      "Epoch: 78, Loss: 0.91, Acc@1: 81.25, Acc@5: 100.00\n",
      "Epoch: 79, Loss: 0.91, Acc@1: 78.12, Acc@5: 96.88\n",
      "Epoch: 80, Loss: 0.90, Acc@1: 84.38, Acc@5: 93.75\n",
      "Epoch: 81, Loss: 0.92, Acc@1: 59.38, Acc@5: 93.75\n",
      "Epoch: 82, Loss: 0.87, Acc@1: 78.12, Acc@5: 93.75\n",
      "Epoch: 83, Loss: 0.86, Acc@1: 62.50, Acc@5: 93.75\n",
      "Epoch: 84, Loss: 0.86, Acc@1: 84.38, Acc@5: 96.88\n",
      "Epoch: 85, Loss: 0.86, Acc@1: 78.12, Acc@5: 96.88\n",
      "Epoch: 86, Loss: 0.87, Acc@1: 81.25, Acc@5: 96.88\n",
      "Epoch: 87, Loss: 0.84, Acc@1: 81.25, Acc@5: 100.00\n",
      "Epoch: 88, Loss: 0.83, Acc@1: 84.38, Acc@5: 100.00\n",
      "Epoch: 89, Loss: 0.85, Acc@1: 65.62, Acc@5: 90.62\n",
      "Epoch: 90, Loss: 0.83, Acc@1: 78.12, Acc@5: 100.00\n",
      "Epoch: 91, Loss: 0.86, Acc@1: 68.75, Acc@5: 87.50\n",
      "Epoch: 92, Loss: 0.84, Acc@1: 75.00, Acc@5: 96.88\n",
      "Epoch: 93, Loss: 0.82, Acc@1: 78.12, Acc@5: 96.88\n",
      "Epoch: 94, Loss: 0.82, Acc@1: 81.25, Acc@5: 93.75\n",
      "Epoch: 95, Loss: 0.80, Acc@1: 71.88, Acc@5: 96.88\n",
      "Epoch: 96, Loss: 0.85, Acc@1: 87.50, Acc@5: 100.00\n",
      "Epoch: 97, Loss: 0.83, Acc@1: 75.00, Acc@5: 96.88\n",
      "Epoch: 98, Loss: 0.82, Acc@1: 56.25, Acc@5: 96.88\n",
      "Epoch: 99, Loss: 0.82, Acc@1: 81.25, Acc@5: 93.75\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "def training(model, train_dl, num_epochs):\n",
    "  # Loss Function, Optimizer and Scheduler\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "  scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001,\n",
    "                                                steps_per_epoch=int(len(train_dl)),\n",
    "                                                epochs=num_epochs,\n",
    "                                                anneal_strategy='linear')\n",
    "\n",
    "  # Repeat for each epoch\n",
    "  for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    correct_prediction = 0\n",
    "    total_prediction = 0\n",
    "\n",
    "    # Repeat for each batch in the training set\n",
    "    for i, data in enumerate(train_dl):\n",
    "        # Get the input features and target labels, and put them on the GPU\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # Normalize the inputs\n",
    "        inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
    "        inputs = (inputs - inputs_m) / inputs_s\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "\n",
    "\n",
    "        # Keep stats for Loss and Accuracy\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        \n",
    "        \n",
    "        acc1, acc5 = accuracy(outputs, labels, topk=(1, 5))\n",
    "        # Get the predicted class with the highest score\n",
    "        _, prediction = torch.max(outputs,1)\n",
    "        # Count of predictions that matched the target label\n",
    "        correct_prediction += (prediction == labels).sum().item()\n",
    "        total_prediction += prediction.shape[0]\n",
    "\n",
    "        #if i % 50 == 0:    # print every 50 mini-batches\n",
    "            #print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 10))\n",
    "    \n",
    "    # Print stats at the end of the epoch\n",
    "    num_batches = len(train_dl)\n",
    "    avg_loss = running_loss / num_batches\n",
    "    acc = correct_prediction/total_prediction\n",
    "    wandb.log({'Train Loss': loss, 'Train Acc': acc})\n",
    "    print(f'Epoch: {epoch}, Loss: {avg_loss:.2f}, Acc@1: {acc1[0].item():.2f}, Acc@5: {acc5[0].item():.2f}')\n",
    "\n",
    "\n",
    "  print('Finished Training')\n",
    "  \n",
    "num_epochs=100\n",
    "training(myModel, train_dl, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae3cc1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.57, Total items: 400\n"
     ]
    }
   ],
   "source": [
    "# Validation\n",
    "\n",
    "def inference (model, val_dl):\n",
    "  correct_prediction = 0\n",
    "  total_prediction = 0\n",
    "\n",
    "  # Disable gradient updates\n",
    "  with torch.no_grad():\n",
    "    for data in val_dl:\n",
    "      # Get the input features and target labels, and put them on the GPU\n",
    "      inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "      # Normalize the inputs\n",
    "      inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
    "      inputs = (inputs - inputs_m) / inputs_s\n",
    "\n",
    "      # Get predictions\n",
    "      outputs = model(inputs)\n",
    "\n",
    "      # Get the predicted class with the highest score\n",
    "      _, prediction = torch.max(outputs,1)\n",
    "      # Count of predictions that matched the target label\n",
    "      correct_prediction += (prediction == labels).sum().item()\n",
    "      total_prediction += prediction.shape[0]\n",
    "    \n",
    "  acc = correct_prediction/total_prediction\n",
    "  print(f'Accuracy: {acc:.2f}, Total items: {total_prediction}')\n",
    "\n",
    "# Run inference on trained model with the validation set\n",
    "inference(myModel, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bc6dcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the model weights and parameters to be used on the edge\n",
    "\n",
    "# Move the model to CPU\n",
    "myModel_CPU = myModel.to('cpu')\n",
    "\n",
    "#Export model in TorchScript Format\n",
    "model_scripted = torch.jit.script(myModel_CPU)\n",
    "model_scripted.save('CNN_Model_cpu.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
