{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a51b4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl \n",
    "from PIL import Image\n",
    "import lmdb\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import *\n",
    "import torchaudio\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchsummary import summary\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f160f644",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b0f998",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb5cf46",
   "metadata": {},
   "source": [
    "### preprocessing with 2 channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01437f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_spectrogram(values, clip, entries):\n",
    "    sampling_rate = 44100\n",
    "    \n",
    "    for data in entries:\n",
    "\n",
    "        num_channels = 3\n",
    "        window_sizes = [25, 50, 100]\n",
    "        hop_sizes = [10, 25, 50]\n",
    "        centre_sec = 2.5\n",
    "\n",
    "        specs = []\n",
    "        for i in range(num_channels):\n",
    "            window_length = int(round(window_sizes[i]*sampling_rate/1000))\n",
    "            hop_length = int(round(hop_sizes[i]*sampling_rate/1000))\n",
    "\n",
    "            clip = torch.Tensor(clip)\n",
    "            spec = torchaudio.transforms.MelSpectrogram(sample_rate=sampling_rate, n_fft=4410, win_length=window_length, hop_length=hop_length, n_mels=128)(clip)\n",
    "            eps = 1e-6\n",
    "            spec = spec.numpy()\n",
    "            spec = np.log(spec+ eps)\n",
    "            spec = np.asarray(torchvision.transforms.Resize((128, 250))(Image.fromarray(spec)))\n",
    "            specs.append(spec)\n",
    "        new_entry = {}\n",
    "        new_entry[\"audio\"] = clip.numpy()\n",
    "        new_entry[\"values\"] = np.array(specs)\n",
    "        new_entry[\"target\"] = data[\"target\"]\n",
    "        values.append(new_entry)\n",
    "\n",
    "def extract_features(audios):\n",
    "    data_dir = 'DataSci251_FinalProject/DataSet/ESC-50-master/audio'\n",
    "    sampling_rate = 44100\n",
    "    \n",
    "    audio_names = list(audios.filename.unique())\n",
    "    values = []\n",
    "    for audio in audio_names:\n",
    "        clip, sr = librosa.load(\"{}/{}\".format(data_dir, audio), sr=sampling_rate)\n",
    "        entries = audios.loc[audios[\"filename\"]==audio].to_dict(orient=\"records\")\n",
    "        extract_spectrogram(values, clip, entries)\n",
    "        print(\"Finished audio {}\".format(audio))\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe2accf",
   "metadata": {},
   "outputs": [],
   "source": [
    "audios = pd.read_csv('DataSci251_FinalProject/DataSet/ESC-50-master/meta/esc50.csv', skipinitialspace=True)\n",
    "num_folds = 5\n",
    "\n",
    "store_dir = 'DataSci251_FinalProject/DataSet/ESC-50-master/store_spectograms_2'\n",
    "\n",
    "for i in range(1, num_folds+1):\n",
    "    training_audios = audios.loc[audios[\"fold\"]!=i]\n",
    "    validation_audios = audios.loc[audios[\"fold\"]==i]\n",
    "\n",
    "    training_values = extract_features(training_audios)\n",
    "    with open(\"{}training128mel{}.pkl\".format(store_dir, i),\"wb\") as handler:\n",
    "        pkl.dump(training_values, handler, protocol=pkl.HIGHEST_PROTOCOL)\n",
    "\n",
    "    validation_values = extract_features(validation_audios)\n",
    "    with open(\"{}validation128mel{}.pkl\".format(store_dir, i),\"wb\") as handler:\n",
    "        pkl.dump(validation_values, handler, protocol=pkl.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0229bb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4407ffb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelSpectrogram(object):\n",
    "    def __init__(self, bins, mode, dataset):\n",
    "        self.window_length = [25, 50, 100]\n",
    "        self.hop_length = [10, 25, 50]\n",
    "        self.fft = 4410\n",
    "        self.melbins = bins\n",
    "        self.mode = mode\n",
    "        self.sr = 44100\n",
    "        self.length = 250\n",
    "    def __call__(self, value):\n",
    "        sample = value\n",
    "        limits = ((-2, 2), (0.9, 1.2))\n",
    "\n",
    "        if self.mode==\"train\":\n",
    "            pitch_shift = np.random.randint(limits[0][0], limits[0][1] + 1)\n",
    "            time_stretch = np.random.random() * (limits[1][1] - limits[1][0]) + limits[1][0]\n",
    "            new_audio = librosa.effects.time_stretch(y = librosa.effects.pitch_shift(y = sample, sr = self.sr, n_steps = pitch_shift), rate = time_stretch)\n",
    "        else:\n",
    "            pitch_shift = 0\n",
    "            time_stretch = 1\n",
    "            new_audio = sample\n",
    "        specs = []\n",
    "        for i in range(len(self.window_length)):\n",
    "            clip = torch.Tensor(new_audio)\n",
    "\n",
    "            window_length = int(round(self.window_length[i]*self.sr/1000))\n",
    "            hop_length = int(round(self.hop_length[i]*self.sr/1000))\n",
    "            spec = torchaudio.transforms.MelSpectrogram(sample_rate=self.sr, n_fft=self.fft, win_length=window_length, hop_length=hop_length, n_mels=self.melbins)(clip)\n",
    "            eps = 1e-6\n",
    "            spec = spec.numpy()\n",
    "            spec = np.log(spec+ eps)\n",
    "            spec = np.asarray(torchvision.transforms.Resize((128, self.length))(Image.fromarray(spec)))\n",
    "            specs.append(spec)\n",
    "        specs = np.array(specs).reshape(-1, 128, self.length)\n",
    "        specs = torch.Tensor(specs)\n",
    "        return specs\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, pkl_dir, dataset_name, transforms=None):\n",
    "        self.transforms = transforms\n",
    "        self.data = []\n",
    "        self.length = 250\n",
    "        with open(pkl_dir, \"rb\") as f:\n",
    "            self.data = pkl.load(f)\n",
    "    def __len__(self):\n",
    "        if self.transforms.mode == \"train\":\n",
    "            return 2*len(self.data)\n",
    "        else:\n",
    "            return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.data):\n",
    "            print(\"getting item if\")\n",
    "            new_idx = idx - len(self.data)\n",
    "            entry = self.data[new_idx]\n",
    "            if self.transforms:\n",
    "                values = self.transforms(entry[\"audio\"])\n",
    "        else:\n",
    "            print(\"getting item else\")\n",
    "            entry = self.data[idx]\n",
    "            values = torch.Tensor(entry[\"values\"].reshape(-1, 128, self.length))\n",
    "        target = torch.LongTensor([entry[\"target\"]])\n",
    "        print(\"returning values and target\")\n",
    "        return (values, target)\n",
    "\n",
    "def fetch_dataloader(pkl_dir, dataset_name, batch_size, num_workers, mode):\n",
    "    transforms = MelSpectrogram(128, mode, dataset_name)\n",
    "    dataset = AudioDataset(pkl_dir, dataset_name, transforms=transforms)\n",
    "    print(type(dataset[0]))\n",
    "    print(dataset[0][0].size())\n",
    "    print(dataset[0][1].size())\n",
    "    print(dataset[1][0].size())\n",
    "    print(dataset[1][1].size())\n",
    "    dataloader = DataLoader(dataset,shuffle=True, batch_size=batch_size, num_workers=num_workers)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb11065",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "\n",
    "# Audio Classification Model\n",
    "\n",
    "class AudioClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        conv_layers = []\n",
    "\n",
    "        # First Convolution Block with Relu and Batch Norm. Use Kaiming Initialization\n",
    "        self.conv1 = nn.Conv2d(3, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        init.kaiming_normal_(self.conv1.weight, a=0.1)\n",
    "        self.conv1.bias.data.zero_()\n",
    "        conv_layers += [self.conv1, self.relu1, self.bn1]\n",
    "\n",
    "        # Second Convolution Block\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        init.kaiming_normal_(self.conv2.weight, a=0.1)\n",
    "        self.conv2.bias.data.zero_()\n",
    "        conv_layers += [self.conv2, self.relu2, self.bn2]\n",
    "\n",
    "        # Second Convolution Block\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        init.kaiming_normal_(self.conv3.weight, a=0.1)\n",
    "        self.conv3.bias.data.zero_()\n",
    "        conv_layers += [self.conv3, self.relu3, self.bn3]\n",
    "\n",
    "        # Second Convolution Block\n",
    "        self.conv4 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        init.kaiming_normal_(self.conv4.weight, a=0.1)\n",
    "        self.conv4.bias.data.zero_()\n",
    "        conv_layers += [self.conv4, self.relu4, self.bn4]\n",
    "\n",
    "        # Linear Classifier\n",
    "        self.ap = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.lin = nn.Linear(in_features=64, out_features=50)\n",
    "\n",
    "        # Wrap the Convolutional Blocks\n",
    "        self.conv = nn.Sequential(*conv_layers)\n",
    " \n",
    "    # Forward pass\n",
    "    def forward(self, x):\n",
    "        # Run the convolutional blocks\n",
    "        x = self.conv(x)\n",
    "\n",
    "        # Adaptive pool and flatten for input to linear layer\n",
    "        x = self.ap(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # Linear layer\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # Final output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1c9a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "dataset_name = \"ESC\"\n",
    "data_dir = \"DataSci251_FinalProject/DataSet/ESC-50-master/store_spectograms_2\"\n",
    "dataaug = True\n",
    "pretrained = True\n",
    "scheduler = True,\n",
    "model = \"resnet\"\n",
    "batch_size = 32\n",
    "num_workers = 8\n",
    "epochs = 70\n",
    "lr = 1e-4\n",
    "weight_decay = 1e-3\n",
    "num_folds = 5\n",
    "checkpoint_dir = \"DataSci251_FinalProject/DataSet/ESC-50-master/checkpoint_dir_cnn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6996b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=\"audio_densenet\",\n",
    "    # Track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"model\": 'cnn',\n",
    "        \"batch_size\":batch_size,\n",
    "        \"learning_rate\": lr,\n",
    "        \"epochs\": epochs,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ea7810",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunningAverage():\n",
    "    def __init__(self):\n",
    "        self.total = 0\n",
    "        self.steps = 0\n",
    "    def update(self, loss):\n",
    "        self.total += loss\n",
    "        self.steps += 1\n",
    "    def __call__(self):\n",
    "        return (self.total/float(self.steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1805520c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, split, checkpoint):\n",
    "    filename = os.path.join(checkpoint, 'last{}.pth.tar'.format(split))\n",
    "    if not os.path.exists(checkpoint):\n",
    "        print(\"Checkpoint Directory does not exist\")\n",
    "        os.mkdir(checkpoint)\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, os.path.join(checkpoint, \"model_best_{}.pth.tar\".format(split)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4d99e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, data_loader, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    loss_avg = RunningAverage()\n",
    "\n",
    "    with tqdm(total=len(data_loader)) as t:\n",
    "        for batch_idx, data in enumerate(data_loader):\n",
    "            inputs = data[0].to(device)\n",
    "            target = data[1].squeeze(1).to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = loss_fn(outputs, target)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_avg.update(loss.item())\n",
    "\n",
    "            t.set_postfix(loss='{:05.3f}'.format(loss_avg()))\n",
    "            t.update()\n",
    "    return loss_avg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5a0434",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, device, test_loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(test_loader):\n",
    "            inputs = data[0].to(device)\n",
    "            target = data[1].squeeze(1).to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "    return (100*correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74156744",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, device, train_loader, val_loader, optimizer, loss_fn, writer, epochs, checkpoint_dir, split, scheduler=None):\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        avg_loss = train(model, device, train_loader, optimizer, loss_fn)\n",
    "\n",
    "        acc = evaluate(model, device, val_loader)\n",
    "        print(\"Epoch {}/{} Loss:{} Valid Acc:{}\".format(epoch, epochs, avg_loss, acc))\n",
    "        \n",
    "        wandb.log({\"accuracy\": acc, \"loss\": avg_loss})\n",
    "\n",
    "        is_best = (acc > best_acc)\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        if is_best:\n",
    "            best_acc = acc\n",
    "            filename = os.path.join(\"{}\".format(checkpoint_dir), 'myModel_export.pt')\n",
    "            model_cpu = model.to('cpu')\n",
    "            model_scripted = torch.jit.script(model_cpu)\n",
    "            model_scripted.save(filename)\n",
    "            \n",
    "            model = model.to('cuda')\n",
    "\n",
    "        save_checkpoint({\"epoch\": epoch + 1,\n",
    "                               \"model\": model.state_dict(),\n",
    "                               \"optimizer\": optimizer.state_dict()}, is_best, split, \"{}\".format(checkpoint_dir))\n",
    "        writer.add_scalar(\"data{}/trainingLoss{}\".format(dataset_name, split), avg_loss, epoch)\n",
    "        writer.add_scalar(\"data{}/valLoss{}\".format(dataset_name, split), acc, epoch)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f014a877",
   "metadata": {},
   "outputs": [],
   "source": [
    "## need config path\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for i in range(1, num_folds+1):\n",
    "    if dataaug:\n",
    "        train_loader = fetch_dataloader( \"{}training128mel{}.pkl\".format(data_dir, i), \"ESC\", batch_size, num_workers, 'train')\n",
    "        val_loader = fetch_dataloader(\"{}validation128mel{}.pkl\".format(data_dir, i), \"ESC\", batch_size, num_workers, 'validation')\n",
    "    else:\n",
    "        print(\"something wrong\")\n",
    "\n",
    "    writer = SummaryWriter(comment=\"ESC\")\n",
    "    model = AudioClassifier().to(device)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    if scheduler:\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 30, gamma=0.1)\n",
    "    else:\n",
    "        scheduler = None\n",
    "\n",
    "    train_and_evaluate(model, device, train_loader, val_loader, optimizer, loss_fn, writer,epochs, checkpoint_dir, i, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d77eb53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
